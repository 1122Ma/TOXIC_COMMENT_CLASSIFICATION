{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input,  Activation\n",
    "from keras.layers import  GlobalMaxPool1D, Dropout\n",
    "from keras import  optimizers, layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = stopwords.words('english')\n",
    "import html\n",
    "import unicodedata\n",
    "import html\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Uploading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the data from file \n",
    "train_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 10 samples from the file\n",
    "train_df.sample(10, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cond lable == toxic bering it \n",
    "train_df.loc[train_df['toxic'] == 1].sample(10, random_state=1234)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Cleaning the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_chars(text):\n",
    "    re1 = re.compile(r'  +')\n",
    "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x1))\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "### search for it \n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "    \"\"\"\n",
    "    :param words:\n",
    "    :type words:\n",
    "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    or\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    :type stop_words:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "#####\n",
    "#def stem_words(words):\n",
    "   # \"\"\"Stem words in text\"\"\"\n",
    "    #stemmer = PorterStemmer()\n",
    "    #return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize words in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
    "\n",
    "def text2words(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def cleaning_text( text):\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    words = text2words(text)\n",
    "    words = remove_stopwords(words, stop_words)\n",
    "    #words = stem_words(words)# Either stem ovocar lemmatize\n",
    "    words = lemmatize_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning in all comment_text in dataset مرور  (understand  lambda )\n",
    "train_df['comment_text'] = train_df['comment_text'].apply(lambda x: cleaning_text(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning comment in x \n",
    "X = train_df['comment_text'] \n",
    "# y conten the class for a sntences == lables\n",
    "y = train_df[train_df.columns[2:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a TextVectorization layer\n",
    "# output_sequence_length every sentance we take from the dataset we convert to 600 length\n",
    "# output_mode to convert each single word to integer\n",
    "vocab=20000\n",
    "vectorizer = TextVectorization(max_tokens=vocab,\n",
    "                               output_sequence_length=600,\n",
    "                               output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apabt and teach vectorizer our vocabulary\n",
    "vectorizer.adapt(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer the words , convert the words to int then save it in vectorized_text\n",
    "vectorized_text = vectorizer(X.values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MCSHBAP -basic data pipeline generation set # Tow ways to instantiate the data pipline (from_tensor slices) or (list_file)\n",
    "# pass through our data that just we created ( vectorized_text is the input features ) & ( y is the target lable)\n",
    "#MCSHBAP - map, chache, shuffle, batch, prefetch  from_tensor_slices, list_file\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y))# creating the dataset\n",
    "dataset = dataset.cache()# subsequent data pre_processing\n",
    "dataset = dataset.shuffle(160000)# how large our buffer needs to be \n",
    "dataset = dataset.batch(16)# have each batch represented  as a series of 16 samples \n",
    "dataset = dataset.prefetch(8) # helps bottlenecks\n",
    "#to build data pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.take(int(len(dataset)*.7))\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "filters = 150\n",
    "kernel_size = 3\n",
    "hidden_dims = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Embedding Layer\n",
    "model = Sequential()\n",
    "# Create the embedding layer \n",
    "model.add(Embedding(vocab+1, 32))\n",
    "#model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters, kernel_size, activation = 'relu'))\n",
    "model.add(MaxPooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, activation = 'relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation = 'sigmoid'))\n",
    "\n",
    "# Display the model structure\n",
    "model.summary() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = error function , optimize the model  \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model , epochs the num of traning and its random value \n",
    "batch_size = 64\n",
    "history = model.fit(train , epochs=70 , batch_size = batch_size , validation_data=val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: Preducation & Evaulation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    # Unpack the batch \n",
    "    # X_true = comment in text \n",
    "    # y_true = label for these comment \n",
    "    # batch = display comment and lebal\n",
    "    X_true,y_true = batch\n",
    "    \n",
    "    # Make a prediction \n",
    "    # inter the x_true = comment and see the preduct come with val\n",
    "    yhat = model.predict(X_true)\n",
    "    \n",
    "    # read about it ***\n",
    "    # Flatten the predictions\n",
    "    y_true = y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "    \n",
    "    # compare betwenn the result y_true which is the true one  and yhat which the predect one from the model \n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision: {pre.result().numpy()}, Recall:{re.result().numpy()}, Accuracy:{acc.result().numpy()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('toxicityNLP_Project10.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('toxicityNLP_Project10.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_comment(comment):\n",
    "    vectorized_comment = vectorizer(comment)\n",
    "    results = model.predict(np.expand_dims(vectorized_comment,0))\n",
    "    \n",
    "    text = ''\n",
    "    for idx, col in enumerate(train_df.columns[2:]):\n",
    "        if results[0][idx]>0.5:  #the result of our predect if > 0.5 is toxic \n",
    "           text += '{}\\n'.format(col)\n",
    "    if text == '':\n",
    "      return 'Your comment dose not have any toxic'\n",
    "    else: \n",
    "      return 'Your comment is : \\n '+text + '( You should choose your words carefully,please be kind ! )'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interface = gr.Interface(fn=score_comment, \n",
    "                         inputs=gr.inputs.Textbox(lines=2, placeholder='Comment to score'),\n",
    "                         outputs='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to run our interfce  \n",
    "interface.launch(share=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e4aaafcc86466ccd00025284bd9144033731ce0b2f29ba03627e546a2099be9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
